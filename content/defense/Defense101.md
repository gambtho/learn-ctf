---
title: "Defense 101"
date: 2017-06-24T04:46:35Z
---

# Welcome to Defense 101

In this first scenario, we will become familiar with Elasticsearch and Kibana as well as get our feet wet digging into a well known banking trojan.  We'll start off easy and progress into increasingly challenging questions as we go.

## Your Elastic Stack Instance

You should now have a working instance of Elastic Stack running locally.  If you plan to continue to use that as your analysis platform, then you will find the resources below helpful.  If you instead plan to use some other tool for log analysis, then you are certainly welcome to.

The only things you should have to do after cloning [the repo](https://github.com/egaus/greyhatctf_elk), installing docker, and docker-compose, is to:

1. Change to the root directory (e.g. cd greyhatctf_elk directory)
1. Run `docker-compose build`
1. Then run `docker-compose up`
1. Finally, you need to load logs for this scenario.  To do so, copy the file scenario_01.tar.gz from the "scenarios" directory into the "DROP_SCENARIO_FILES_HERE" directory.  After this is done the file you copied will be processed, timestamps will be automatically adjusted, and the logs will be loaded into your [Elastic Stack instance](http://localhost:5601).  The file scenario_01.tar.gz will be deleted from the DROP_SCENARIO_FILES_HERE directory, so be sure to copy, not move, the file or you'll need to re-download the content from Github.

## Start Exploring the Logs

You should now have a working instance of Kibana.  Upon loading logs following the instructions above, you need to setup the index mappings.  This will largely be done for you.  Simply click on the Management area of Kibana (you will probably be redirected there automatically).  

![Kibana Image](/defense/navigate_to_kibana.png)

Then click the "Create" button towards the bottom of the screen and you should see that Kibana has already mapped the fields for you.

![Kibana Image](/defense/kibana_index_mapping.png)

Next click on the Discover tab and you should see some logs.  In the upper right-hand corner of the screen is a time-picker.  At ingest, the logs have been modified for you to have a beginning time of 10 hours ago.  You will likely work on this challenge for a few hours, so it is recommended that you set the time picker by clicking on "Quick" > "Last 24 hours" and then you won't need to adjust the date / time as much as you perform your analysis.

![Kibana Image](/defense/discover_time_picker.png)

## Kibana Discover

You know where to set the time picker, but now you want to query the event logs during the time period in question.  In the search bar you can enter queries in the Lucene query language.  If you are not familiar, spend a little time looking over [Lucene Syntax](https://lucene.apache.org/core/2_9_4/queryparsersyntax.html) or [Kibana Queries In Depth](https://www.timroes.de/2016/05/29/elasticsearch-kibana-queries-in-depth-tutorial/).

The logs themselves were generated by Bro NSM.  We'll cover some important concepts, log types and fields here, but you will no doubt want to become more familiar with [Bro log fields](https://www.bro.org/sphinx/script-reference/log-files.html) and their meaning.

You should have multiple log types loaded at this point.  As you explore, we recommend starting with something familiar by running a search for something like **type:bro_http**.  This is our first Lucene query and it is a term search for the text "bro_http" against the field called "type".  This search will return all of the http events that bro identified and provided event metadata for.

## Kibana Introduction

If you are relatively new to Kibana, click on the Kibana image below to get access to several Kibana training videos to get you started making powerful searches and insightful visualizations.

[![Kibana Training Resources](/defense/kibana_logo.png)](https://p.brightact.com/p/1477331815405265)


## Check Out Some Bro logs

Now that you have logs loaded and you have reviewed some of the Kibana usage material, we can start exploring Bro logs in some detail.  Bro Network Security Monitor (NSM) allow analysts to review logs and determine what happened on the network.  Bro has a wide array of [protocol analyzers](https://www.bro.org/sphinx/script-reference/proto-analyzers.html).

The first log type we'll look at are conn logs.  These logs are similar to netflow logs, where we are given the source and destination IP addresses (in Bro, these are id.orig_h and id.resp_h), source and destination ports (id.orig_p and id.resp_p), and size and duration of transfer.  Go ahead and search for them using the search **type:bro_conn**.  Bro conn logs are generated based on network sessions.  The **uid** is a very important field in the Bro logging framework.  When a network connection is identified by Bro, based on a series of configurable timers, Bro will decide when to log network session events.  If the protocol is determined to be one with a protocol analyzer defined, Bro will additionally run the script for the analyzer.  In the case of http, bro will create a bro_http log in and link it to the bro_conn log using the **uid** value from the session.  We can use the uid value to aggregate context about a network connection.

Let's look at an example.  Search for **type:bro_conn AND uid:CiODE54vrEjjnpk5Vk**.  This is a conn event and we are given information at the session level.  If we change our search to look just for the particular uid (**uid:CiODE54vrEjjnpk5Vk**), we'll see the other metadata bro recorded about this network "connection" and get the additional details about the associated DNS request and response.

Overview of [Bro log types and fields](http://gauss.ececs.uc.edu/Courses/c6055/pdf/bro_log_vars.pdf).  

## Getting started with the exercises and Bro logs

<INSERT GETTING STARTED VIDEO HERE>

## Apply Threat Intelligence

Judicious use of threat intelligence can significantly enhance a teams ability to identify attacks and consistently catch the same adversary in the future even when some of their tools or methods are updated for the next campaign.  Take a moment to review these two articles about the entire attack scenario involving Vawtrak.  Pay special attention to how Vawtrak works and what it might look like in network event logs.

* https://www.fidelissecurity.com/threatgeek/2016/05/vawtrak-trojan-bank-it-evolving
* https://www.proofpoint.com/us/threat-insight/post/Vawtrak-UrlZone-Banking-Trojans-Target-Japan

Scenario: Yesterday your organization saw a wave of phishing emails with a successful lure that unfortunately many fell for.  Those who were on premise were protected because the malware failed to download due to on-site preventative controls.  Unfortunately, those who work remotely and some 3rd party contractors were unprotected.  Some of these employees are now likely on the network today and we need to identify systems infected with the Vawtrak banking trojan.  Your mission is to explore the sample logs, determine how to identify Vawtrak is active to scope the infections, and detect any future infections.

<a href="/defense/getting_started/"><button type="button" class="btn btn-Primary btn-arrow-left">Prev Article</button></a>&nbsp&nbsp&nbsp&nbsp<a href="/defense/defense201/"><button type="button" class="btn btn-Primary btn-arrow-right">Next Article</button></a>
