---
title: "Scenario 01: Vawtrak"
date: 2017-06-24T04:46:35Z
---

# Welcome to Scenario 01

In this first scenario, we will become familiar with Elasticsearch and Kibana as well as get our feet wet digging into a well known banking trojan.  We'll start off easy and progress into increasingly challenging questions as we go.

## Your Elastic Stack Instance

You should now have a working instance of Elastic Stack running locally.  If you plan to continue to use that as your analysis platform, then you will find the resources below helpful.  If you instead plan to use some other tool for log analysis, then you are certainly welcome to.

The only things you should have to do after cloning [the repo](https://github.com/egaus/greyhatctf_elk), installing docker, and docker-compose, is to:

#) Change to the root directory (e.g. cd greyhatctf_elk directory)
#) Run `docker-compose build`
#) Then run `docker-compose up`
#) Finally, you need to load logs for this scenario.  To do so, copy the file scenario_01.tar.gz from the "scenarios" directory into the "DROP_SCENARIO_FILES_HERE" directory.  After this is done the file you copied will be processed, timestamps will be automatically adjusted, and the logs will be loaded into your [Elastic Stack instance](http://localhost:5601).  The file scenario_01.tar.gz will be deleted from the DROP_SCENARIO_FILES_HERE directory, so be sure to copy, not move, the file or you'll need to re-download the content from Github.

## Start Exploring the Logs

You should now have a working instance of Kibana.  Upon loading logs following the instructions above, you need to setup the index mappings.  This will largely be done for you.  Simply click on the Management area of Kibana (you will probably be redirected there automatically).  

![Kibana Image](/defense/navigate_to_kibana.png)

Then click the "Create" button towards the bottom of the screen and you should see that Kibana has already mapped the fields for you.

![Kibana Image](/defense/kibana_index_mapping.png)

Next click on the Discover tab and you should see some logs.  In the upper right-hand corner of the screen is a time-picker.  At ingest, the logs have been modified for you to have a beginning time of 10 hours ago.  You will likely work on this challenge for a few hours, so it is recommended that you set the time picker to Quick > Last 24 hours and then you won't need to adjust the date / time as much as you perform your analysis.

![Kibana Image](/defense/discover_time_picker.png)

## Discover Area

You know where to set the time picker, but you may not be familiar with the log format or the query language.  In the search bar you can enter queries in the Lucene query language.  If you are not familiar, spend a little time looking over [Lucene Syntax](https://lucene.apache.org/core/2_9_4/queryparsersyntax.html).

The logs themselves were generated by Bro NSM.  We'll cover some important concepts, log types and fields here, but you will no doubt want to become more familiar with [Bro log fields](https://www.bro.org/sphinx/script-reference/log-files.html) and their meaning.

You should have multiple log types loaded at this point.  As you explore, we recommend starting with something familiar by running a search for something like \_type:bro_http.  This is our first Lucene query and it is a term search for the text "bro_http" against the field called "\_type".

## Visualize Area
